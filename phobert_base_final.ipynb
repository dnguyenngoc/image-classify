{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a39c6e48-f6c1-4232-9d61-fde4fabb80be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('./backend/app/')\n",
    "import torch\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "from pprint import pprint\n",
    "from helpers.tesseract_utils import Tesseract\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize\n",
    "from helpers import text_utils\n",
    "from helpers import es_utils\n",
    "from helpers import image_utils\n",
    "from helpers import scanner\n",
    "from elasticsearch import Elasticsearch\n",
    "from sklearn.utils import shuffle\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72e93584-efce-4223-b271-caa75ddc777a",
   "metadata": {},
   "outputs": [],
   "source": [
    "LEN_FEATURE = 768\n",
    "LEN_TOKEN = 140\n",
    "INDEX_NAME = 'image-classify-base'\n",
    "ES_HOST = '10.1.32.130'\n",
    "ES_PORT = '9200'\n",
    "PATH_DATASET = './datasets/image_classify/main.xlsx'\n",
    "PATH_TEST = './datasets/image_classify/test.xlsx'\n",
    "\n",
    "CLASSES = {\n",
    "    1: 'discharge record',\n",
    "    2: 'driver licence',\n",
    "    3: 'invoice',\n",
    "    4: 'resume',\n",
    "    5: 'vehicle certificate',\n",
    "    6: 'degree of bachelor'\n",
    "    \n",
    "}\n",
    "\n",
    "STOPWORDS = set([\n",
    "    '\\\\', '(', ')', ':', '.', ';', ',', '\\\\\\\\', '\\\\\\\\\\\\', '-', '%', '`', '—-', '?', '——', '--', '@',  '[', ']', '.....', '``', 'đụ', 'đéo'\n",
    "#     'cộng', 'hòa', 'xã', 'hội', 'chủ', 'nghĩa', 'việt', 'nam', 'độc', 'lập', 'tự', 'do', 'hạnh', 'phúc'\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95a57589-212a-4122-9dc4-a009125d6acd",
   "metadata": {},
   "outputs": [],
   "source": [
    "phobert = AutoModel.from_pretrained(\"models/phobert-base\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"models/phobert-base\")\n",
    "tesseract = Tesseract(out_type='string')\n",
    "es = Elasticsearch([{'host': ES_HOST, 'port': ES_PORT}])\n",
    "scanner = scanner.ScannerFindContours()\n",
    "\n",
    "\n",
    "def get_class_name(class_id):\n",
    "    return CLASSES[class_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88ed4b2d-a70a-4998-ba12-066fce1f0914",
   "metadata": {},
   "outputs": [],
   "source": [
    "giay_ra_vien = image_utils.load_datasets('../../datasets/image_classify/images/giay_ra_vien/')\n",
    "giay_phep_lai_xe = image_utils.load_datasets('../../datasets/image_classify/images/giay_phep_lai_xe/')\n",
    "hoa_don = image_utils.load_datasets('../../datasets/image_classify/images/hoa_don/')\n",
    "cv = image_utils.load_datasets('../../datasets/image_classify/images/cv/')\n",
    "cavet_xe_may = image_utils.load_datasets('../../datasets/image_classify/images/cavet_xe_may/')\n",
    "bang_dai_hoc = image_utils.load_datasets('../../datasets/image_classify/images/bang_dai_hoc/')\n",
    "\n",
    "image = scanner.process(image_utils.load(cavet_xe_may[2]))\n",
    "# mask, image = image_utils.get_title(image)\n",
    "plt.imshow(image)\n",
    "plt.title(giay_ra_vien[0].split('/')[-1])\n",
    "plt.show()\n",
    "\n",
    "\n",
    "image = image_utils.pre_process(image)\n",
    "plt.imshow(image, cmap = 'gray')\n",
    "# plt.title(giay_ra_vien[0].split('/')[-1])\n",
    "plt.show()\n",
    "\n",
    "\n",
    "text = tesseract.excecute(image)    \n",
    "tokens = word_tokenize(text_utils.text_cleaner(text))\n",
    "tokens = text_utils.fix_tokens(tokens, STOPWORDS)\n",
    "print(tokens[0:140])\n",
    "\n",
    "\n",
    "if len(tokens) >= LEN_TOKEN:\n",
    "    tokens = tokens[0:LEN_TOKEN]\n",
    "else:\n",
    "    for i in range(len(tokens) - LEN_TOKEN):\n",
    "        tokens.append('None')\n",
    "input_ids = torch.tensor([tokenizer.encode(tokens)])\n",
    "with torch.no_grad():\n",
    "    features = phobert(input_ids) \n",
    "es_dim = features['pooler_output'][0].tolist()\n",
    "print(features['pooler_output'][0][0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d69ea82-6c9c-446d-be11-0d6d2d44ff9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_data(data: list, list_data, class_id):\n",
    "    for item in list_data:\n",
    "        data.append([item, class_id])\n",
    "    return data\n",
    "        \n",
    "columns = ['image','class']\n",
    "data_test = []\n",
    "make_data(data_test, giay_ra_vien[10:], 1)\n",
    "make_data(data_test, giay_phep_lai_xe[10:], 2)\n",
    "make_data(data_test, hoa_don[10:], 3)\n",
    "make_data(data_test, cv[10:], 4)\n",
    "make_data(data_test, cavet_xe_may[10:], 5)\n",
    "make_data(data_test, bang_dai_hoc[10:], 6)\n",
    "\n",
    "data_train = []\n",
    "make_data(data_train, giay_ra_vien[0:10], 1)\n",
    "make_data(data_train, giay_phep_lai_xe[0:10], 2)\n",
    "make_data(data_train, hoa_don[0:10], 3)\n",
    "make_data(data_train, cv[0:10], 4)\n",
    "make_data(data_train, cavet_xe_may[0:10], 5)\n",
    "make_data(data_train, bang_dai_hoc[0:10], 6)\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "df_train = pd.DataFrame(data_train, columns=columns)\n",
    "df_train = shuffle(df_train).reset_index(drop=True)\n",
    "df_train['type'] = ['train' for i in range(df_train.shape[0])]\n",
    "\n",
    "df_test = pd.DataFrame(data_test, columns=columns)\n",
    "df_test = shuffle(df_test).reset_index(drop=True)\n",
    "df_test['type'] = ['test' for i in range(df_test.shape[0])]\n",
    "\n",
    "frames = [df_train, df_test]\n",
    "df = pd.concat(frames)\n",
    "\n",
    "\n",
    "def get_list_class(classes):\n",
    "    return [value for key, value in classes.items()]\n",
    "\n",
    "\n",
    "# make chart\n",
    "df_train_sum = df_train.groupby('class').count().reset_index(drop=True)\n",
    "df_test_sum = df_test.groupby('class').count().reset_index(drop=True)\n",
    "df_sum = df.groupby('class').count().reset_index(drop=True)\n",
    "\n",
    "\n",
    "# df_sum['image'] = df_sum['image'].apply(lambda x: x/df.shape[0]*100)\n",
    "\n",
    "\n",
    "\n",
    "X = get_list_class(CLASSES)\n",
    "TRAIN = list(df_train_sum['image'])\n",
    "TEST = list(df_test_sum['image'])\n",
    "\n",
    "plotdata = pd.DataFrame({\n",
    "    \"train\": TRAIN,\n",
    "    \"test\":TEST,\n",
    "    }, \n",
    "    index=X\n",
    ")\n",
    "plotdata.plot(kind=\"bar\")\n",
    "plt.title(\"Train and test dataset\")\n",
    "plt.xlabel(\"Classes\")\n",
    "plt.ylabel(\"Total Sample\")\n",
    "\n",
    "plotdata = pd.DataFrame({\n",
    "    \"data\": list(df_sum['image']),\n",
    "    }, \n",
    "    index=X\n",
    ")\n",
    "plotdata.plot(kind=\"bar\")\n",
    "plt.title(\"Total Datasets\")\n",
    "plt.xlabel(\"Classes\")\n",
    "plt.ylabel(\"Total Sample\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1975ab84-7c8b-44e4-8e1b-0e274512cc62",
   "metadata": {},
   "outputs": [],
   "source": [
    "es_utils.delete_elasticsearch_index(es, INDEX_NAME)\n",
    "es_utils.create_elasticsearch_index(es, INDEX_NAME, LEN_FEATURE)\n",
    "\n",
    "df = df_train\n",
    "len_list_task = len(df['image'])\n",
    "index = 1\n",
    "pbar = tqdm(total=len_list_task,  position=0, leave=False)\n",
    "\n",
    "for i in range(len_list_task):\n",
    "    \n",
    "    image_path = df['image'][i]\n",
    "    class_id = df['class'][i]\n",
    "    image = image_utils.load(image_path)\n",
    "    image = scanner.process(image)\n",
    "    image = image_utils.pre_process(image)\n",
    "    text = tesseract.excecute(image)\n",
    "\n",
    "    tokens = word_tokenize(text_utils.text_cleaner(text))\n",
    "    tokens = text_utils.fix_tokens(tokens, STOPWORDS)\n",
    "    if len(tokens) >= LEN_TOKEN:\n",
    "        tokens = tokens[0:LEN_TOKEN]\n",
    "    else:\n",
    "        for i in range(len(tokens) - LEN_TOKEN):\n",
    "            tokens.append('None')\n",
    "\n",
    "    input_ids = torch.tensor([tokenizer.encode(tokens)])\n",
    "    with torch.no_grad():\n",
    "        features = phobert(input_ids) \n",
    "    es_dim = features['pooler_output'][0].tolist()\n",
    "    es_utils.create_elasticsearch_datasets(es, INDEX_NAME, class_id, CLASSES[class_id] ,image_path, es_dim, index)\n",
    "    index += 1\n",
    "    pbar.update(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b7c538e-ebed-4a8c-b502-ada47d53fb08",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "\n",
    "df =df_test\n",
    "x = 0\n",
    "# confusion_matrix = np.zeros((len(CLASSES), len(CLASSES)))\n",
    "\n",
    "y_actu = []\n",
    "y_pred = []\n",
    "\n",
    "for i in range(len(df['image'])):\n",
    "    image_path = df['image'][i]\n",
    "    class_id = df['class'][i]\n",
    "    \n",
    "    image = scanner.process(image_utils.load(image_path))\n",
    "    image = image_utils.pre_process(image)\n",
    "    text = tesseract.excecute(image)\n",
    " \n",
    "    tokens = word_tokenize(text_utils.text_cleaner(text))\n",
    "    tokens = text_utils.fix_tokens(tokens, STOPWORDS)\n",
    "        \n",
    "    if len(tokens) >= LEN_TOKEN:\n",
    "        tokens = tokens[0:LEN_TOKEN]\n",
    "    else:\n",
    "        for i in range(len(tokens) - LEN_TOKEN):\n",
    "            tokens.append('None')\n",
    "    input_ids = torch.tensor([tokenizer.encode(tokens)])\n",
    "    with torch.no_grad():\n",
    "        features = phobert(input_ids) \n",
    "    es_dim = features['pooler_output'][0].tolist()\n",
    "    pred = es_utils.matching_elasticsearch_index(es, INDEX_NAME, es_dim)\n",
    "   \n",
    "    hits = pred['hits']['hits']\n",
    "    class_pre = hits[0]['_source']['id']\n",
    "    \n",
    "    y_actu.append(class_id)\n",
    "    y_pred.append(class_pre)\n",
    "    \n",
    "    if class_id == class_pre:\n",
    "        x += 1\n",
    "    else:\n",
    "        print(image_path, class_pre, class_id)\n",
    "#     confusion_matrix[class_id -1, class_pre -1] += 1\n",
    "    print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "    \n",
    "print(x/len(df['image']))\n",
    "\n",
    "# #     df_check = pd.json_normalize(hits)\n",
    "# #     df_check = df_check[['_source.id', '_score']]\n",
    "# #     df_check = df_check.rename(columns = {'_source.id': 'id', '_score': 'score'})\n",
    "# #     df_check = df_check.groupby([\"id\"], as_index=False).mean()\n",
    "# #     class_pre = df_check.loc[df_check['score'].idxmax()]['id']\n",
    "\n",
    "df_show= df['class'].apply(get_class_name)\n",
    "df_show.value_counts().plot(kind=\"bar\", title=\"Number of Test Data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08f59c43-37e3-4421-a7cd-ad72c341a6de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sn\n",
    "import numpy as np\n",
    "y = confusion_matrix(y_actu, y_pred)\n",
    "columns = []\n",
    "for key, value in CLASSES.items():\n",
    "    columns.append(value)\n",
    "y = y.astype(np.float32)\n",
    "df_cm = pd.DataFrame(y, index = columns,\n",
    "                     columns =  columns)\n",
    "plt.figure(figsize = (len(CLASSES), len(CLASSES)))\n",
    "sn.heatmap(df_cm, annot=True)\n",
    "\n",
    "for i in range(len(CLASSES)):\n",
    "    total = sum(y[i])\n",
    "    for j in range(len(CLASSES)):\n",
    "        y[i][j] = y[i][j]/total\n",
    "df_cm = pd.DataFrame(y, index = columns,\n",
    "                     columns =  columns)\n",
    "plt.figure(figsize = (len(CLASSES), len(CLASSES)))\n",
    "sn.heatmap(df_cm, annot=True)\n",
    "\n",
    "import seaborn as sns\n",
    "def plot_confusion_matrix(test_y, predict_y, classes):\n",
    "    \"\"\"\n",
    "    This function plots the confusion matrix given predicted and actual values.\n",
    "    \"\"\"\n",
    "    C = confusion_matrix(test_y, predict_y)\n",
    "    # C = 9,9 matrix, each cell (i,j) represents number of points of class i are predicted class j\n",
    "    \n",
    "    A =(((C.T)/(C.sum(axis=1))).T)    \n",
    "    B =(C/C.sum(axis=0))\n",
    "    labels = classes\n",
    "    cmap=sns.light_palette(\"green\")\n",
    "    # representing A in heatmap format\n",
    "    plt.figure(figsize=(17,5))\n",
    "    sns.heatmap(C, annot=True, cmap=cmap, fmt=\".3f\", xticklabels=labels, yticklabels=labels)\n",
    "    plt.xlabel('Predicted Class')\n",
    "    plt.ylabel('Original Class')\n",
    "    plt.show()\n",
    "    print(\"-\"*50, \"Confusion matrix\", \"-\"*50)\n",
    "\n",
    "    plt.figure(figsize=(17,5))\n",
    "    sns.heatmap(B, annot=True, cmap=cmap, fmt=\".3f\", xticklabels=labels, yticklabels=labels)\n",
    "    plt.xlabel('Predicted Class')\n",
    "    plt.ylabel('Original Class')\n",
    "    plt.show()\n",
    "    print(\"-\"*50, \"Precision matrix\", \"-\"*50)\n",
    "    \n",
    "#     print(\"Sum of columns in precision matrix\",B.sum(axis=0))\n",
    "    \n",
    "    # representing B in heatmap format\n",
    "    plt.figure(figsize=(17,5))\n",
    "    sns.heatmap(A, annot=True, cmap=cmap, fmt=\".3f\", xticklabels=labels, yticklabels=labels)\n",
    "    plt.xlabel('Predicted Class')\n",
    "    plt.ylabel('Original Class')\n",
    "    plt.show()\n",
    "    print(\"-\"*50, \"Recall matrix\"    , \"-\"*50)\n",
    "#     print(\"Sum of rows in precision matrix\",A.sum(axis=1))\n",
    "plot_confusion_matrix(y_actu, y_pred, classes)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
