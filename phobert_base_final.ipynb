{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a39c6e48-f6c1-4232-9d61-fde4fabb80be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('./backend/app/')\n",
    "import torch\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "from pprint import pprint\n",
    "# from helpers.tesseract_utils import Tesseract\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize\n",
    "from helpers import text_utils\n",
    "from helpers import es_utils\n",
    "from helpers import image_utils\n",
    "# from helpers import scanner\n",
    "from elasticsearch import Elasticsearch\n",
    "from sklearn.utils import shuffle\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure()\n",
    "import easyocr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72e93584-efce-4223-b271-caa75ddc777a",
   "metadata": {},
   "outputs": [],
   "source": [
    "LEN_FEATURE = 768\n",
    "LEN_TOKEN = 150\n",
    "INDEX_NAME = 'image-classify'\n",
    "ES_HOST = '10.1.32.130'\n",
    "ES_PORT = '9200'\n",
    "PATH_DATASET = './datasets/image_classify/main.xlsx'\n",
    "PATH_TEST = './datasets/image_classify/test.xlsx'\n",
    "\n",
    "CLASSES = {\n",
    "    1: 'Discharge Record',\n",
    "    2: 'Invoice',\n",
    "    3: 'TPFICO form',\n",
    "    4: 'Payment Order',\n",
    "    5: 'Driver Licence',\n",
    "    6: 'Vehicle Registration Certificate',\n",
    "    7: 'Invoice Handwritten',\n",
    "    8: 'Căn Cước Công Dân mặt trước'\n",
    "}\n",
    "\n",
    "\n",
    "STOPWORDS = set([\n",
    "    '\\\\', '(', ')', ':', '.', ';', ',', '\\\\\\\\', '\\\\\\\\\\\\', '-', '%', '`', '—-', '?', '——', '--', '@',  '[', ']', '.....', '``', 'đụ', 'đéo',\n",
    "    'cộng', 'hòa', 'xã', 'hội', 'chủ', 'nghĩa', 'việt', 'nam', 'độc', 'lập', 'tự', 'do', 'hạnh', 'phúc', 'bộ', 'gtvt', 'bọ', 'bỏ', 'bô'\n",
    "    'hĩa', 'phỏ', 'chũ'\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95a57589-212a-4122-9dc4-a009125d6acd",
   "metadata": {},
   "outputs": [],
   "source": [
    "phobert = AutoModel.from_pretrained(\"models/phobert-base\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"models/phobert-base\")\n",
    "# tesseract = Tesseract(out_type='string')\n",
    "es = Elasticsearch([{'host': ES_HOST, 'port': ES_PORT}])\n",
    "# scanner = scanner.ScannerFindContours()\n",
    "reader = easyocr.Reader(['vi', 'en'])\n",
    "\n",
    "\n",
    "def get_class_name(class_id):\n",
    "    return CLASSES[class_id]\n",
    "\n",
    "def make_data(data: list, list_data, class_id):\n",
    "    for item in list_data:\n",
    "        data.append([item, class_id])\n",
    "    return data \n",
    "\n",
    "def get_list_class(classes):\n",
    "    return [value for key, value in classes.items()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88ed4b2d-a70a-4998-ba12-066fce1f0914",
   "metadata": {},
   "outputs": [],
   "source": [
    "# giay_ra_vien = image_utils.load_datasets('../../datasets/image_classify/train/giay_ra_vien/')\n",
    "# giay_phep_lai_xe = image_utils.load_datasets('../../datasets/image_classify/train/giay_phep_lai_xe/')\n",
    "can_cuoc = image_utils.load_datasets('../../datasets/image_classify/train/can_cuoc/')\n",
    "# cv = image_utils.load_datasets('../../datasets/image_classify/train/cv/')\n",
    "# cavet_xe_may = image_utils.load_datasets('../../datasets/image_classify/train/cavet_xe_may/')\n",
    "# bang_dai_hoc = image_utils.load_datasets('../../datasets/image_classify/train/bang_dai_hoc/')\n",
    "tpfico = image_utils.load_datasets('C:\\\\Users\\\\duyng\\\\OneDrive\\\\Máy tính\\\\doc classify\\\\Train\\\\TPFICO_Form')\n",
    "invoice = image_utils.load_datasets('C:\\\\Users\\\\duyng\\\\OneDrive\\\\Máy tính\\\\doc classify\\\\Train\\\\Hoa_Don')\n",
    "invoice_hand = image_utils.load_datasets('C:\\\\Users\\\\duyng\\\\OneDrive\\\\Máy tính\\\\doc classify\\\\Train\\\\Hoa_Don_Hand')\n",
    "# payment = image_utils.load_datasets('C:\\\\Users\\\\duyng\\\\OneDrive\\\\Máy tính\\\\doc classify\\\\Train\\\\Uy_Nhiem_Chi')\n",
    "\n",
    "# from sklearn.utils import shuffle\n",
    "\n",
    "# columns = ['image','class']\n",
    "# data_train = []\n",
    "# make_data(data_train, giay_ra_vien, 1)\n",
    "# make_data(data_train, giay_phep_lai_xe, 2)\n",
    "# make_data(data_train, can_cuoc, 3)\n",
    "# make_data(data_train, cv, 4)\n",
    "# make_data(data_train, cavet_xe_may, 5)\n",
    "# make_data(data_train, bang_dai_hoc, 6)\n",
    "# df_train = pd.DataFrame(data_train, columns=columns)\n",
    "# df_train = shuffle(df_train).reset_index(drop=True)\n",
    "# df_train['type'] = ['train' for i in range(df_train.shape[0])]\n",
    "# print('done!')\n",
    "import PIL\n",
    "from PIL import ImageDraw\n",
    "from PIL import Image\n",
    "from matplotlib import cm\n",
    "import numpy as np\n",
    "image_pil = PIL.Image.open(can_cuoc[0])\n",
    "plt.imshow(image_pil)\n",
    "plt.show()\n",
    "bounds = reader.readtext)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1be1f04-b1f3-4a92-b9e3-0b61f6d286f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Draw bounding boxes\n",
    "def draw_boxes(image, bounds, color='yellow', width=2):\n",
    "    draw = ImageDraw.Draw(image)\n",
    "    for bound in bounds:\n",
    "        p0, p1, p2, p3 = bound[0]\n",
    "        draw.line([*p0, *p1, *p2, *p3, *p0], fill=color, width=width)\n",
    "    return image\n",
    "\n",
    "new_image = draw_boxes(image_pil, bounds)\n",
    "plt.imshow(new_image, cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b5d30ed-dae0-48f5-bbab-06ddd1acfa38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for item in giay_ra_vien:\n",
    "item = can_cuoc[7]\n",
    "pprint(item)\n",
    "image = image_utils.load(item)\n",
    "plt.imshow(image)\n",
    "plt.title(item.split('/')[-1])\n",
    "plt.show()\n",
    "image = image_utils.pre_process(image)\n",
    "plt.imshow(image, cmap = 'gray')\n",
    "plt.title(item.split('/')[-1])\n",
    "plt.show()\n",
    "\n",
    "text = tesseract.excecute(image)    \n",
    "tokens = word_tokenize(text_utils.text_cleaner(text))\n",
    "tokens = text_utils.fix_tokens(tokens, STOPWORDS)\n",
    "print(tokens[0:150])\n",
    "\n",
    "giay_ra_vien = image_utils.load_datasets('../../datasets/image_classify/test/giay_ra_vien/')\n",
    "giay_phep_lai_xe = image_utils.load_datasets('../../datasets/image_classify/test/giay_phep_lai_xe/')\n",
    "can_cuoc = image_utils.load_datasets('../../datasets/image_classify/test/can_cuoc/')\n",
    "cv = image_utils.load_datasets('../../datasets/image_classify/test/cv/')\n",
    "cavet_xe_may = image_utils.load_datasets('../../datasets/image_classify/test/cavet_xe_may/')\n",
    "bang_dai_hoc = image_utils.load_datasets('../../datasets/image_classify/test/bang_dai_hoc/')\n",
    "\n",
    "\n",
    "data_test = []\n",
    "make_data(data_test, giay_ra_vien, 1)\n",
    "make_data(data_test, giay_phep_lai_xe, 2)\n",
    "make_data(data_test, can_cuoc, 3)\n",
    "make_data(data_test, cv, 4)\n",
    "make_data(data_test, cavet_xe_may, 5)\n",
    "make_data(data_test, bang_dai_hoc, 6)\n",
    "\n",
    "df_test = pd.DataFrame(data_test, columns=columns)\n",
    "df_test = shuffle(df_test).reset_index(drop=True)\n",
    "df_test['type'] = ['test' for i in range(df_test.shape[0])]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d69ea82-6c9c-446d-be11-0d6d2d44ff9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "frames = [df_train, df_test]\n",
    "df = pd.concat(frames)\n",
    "\n",
    "# make chart\n",
    "df_train_sum = df_train.groupby('class').count().reset_index(drop=True)\n",
    "df_test_sum = df_test.groupby('class').count().reset_index(drop=True)\n",
    "df_sum = df.groupby('class').count().reset_index(drop=True)\n",
    "\n",
    "\n",
    "X = get_list_class(CLASSES)\n",
    "\n",
    "TRAIN = list(df_train_sum['image'])\n",
    "TEST = list(df_test_sum['image'])\n",
    "\n",
    "pprint(TEST)\n",
    "\n",
    "plotdata = pd.DataFrame({\n",
    "    \"train\": TRAIN,\n",
    "    \"test\":TEST,\n",
    "    }, \n",
    "    index=X\n",
    ")\n",
    "plotdata.plot(kind=\"bar\")\n",
    "plt.title(\"Train and test dataset\")\n",
    "plt.xlabel(\"Classes\")\n",
    "plt.ylabel(\"Total Sample\")\n",
    "\n",
    "plotdata = pd.DataFrame({\n",
    "    \"data\": list(df_sum['image']),\n",
    "    }, \n",
    "    index=X\n",
    ")\n",
    "plotdata.plot(kind=\"bar\")\n",
    "plt.title(\"Total Datasets\")\n",
    "plt.xlabel(\"Classes\")\n",
    "plt.ylabel(\"Total Sample\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1975ab84-7c8b-44e4-8e1b-0e274512cc62",
   "metadata": {},
   "outputs": [],
   "source": [
    "es_utils.delete_elasticsearch_index(es, INDEX_NAME)\n",
    "es_utils.create_elasticsearch_index(es, INDEX_NAME, LEN_FEATURE)\n",
    "\n",
    "df = df_train\n",
    "len_list_task = len(df['image'])\n",
    "index = 1\n",
    "pbar = tqdm(total=len_list_task,  position=0, leave=False)\n",
    "\n",
    "for i in range(len_list_task):\n",
    "    \n",
    "    image_path = df['image'][i]\n",
    "    class_id = df['class'][i]\n",
    "    image = image_utils.load(image_path)\n",
    "    image = image_utils.pre_process(image)\n",
    "    text = tesseract.excecute(image)\n",
    "\n",
    "    tokens = word_tokenize(text_utils.text_cleaner(text))\n",
    "    tokens = text_utils.fix_tokens(tokens, STOPWORDS)\n",
    "    if len(tokens) >= LEN_TOKEN:\n",
    "        tokens = tokens[0:LEN_TOKEN]\n",
    "    else:\n",
    "        for i in range(len(tokens) - LEN_TOKEN):\n",
    "            tokens.append('None')\n",
    "\n",
    "    input_ids = torch.tensor([tokenizer.encode(tokens)])\n",
    "    with torch.no_grad():\n",
    "        features = phobert(input_ids) \n",
    "    es_dim = features['pooler_output'][0].tolist()\n",
    "    es_utils.create_elasticsearch_datasets(es, INDEX_NAME, class_id, CLASSES[class_id] ,image_path, es_dim, index)\n",
    "    index += 1\n",
    "    pbar.update(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b7c538e-ebed-4a8c-b502-ada47d53fb08",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "\n",
    "df =df_test\n",
    "x = 0\n",
    "# confusion_matrix = np.zeros((len(CLASSES), len(CLASSES)))\n",
    "\n",
    "y_actu = []\n",
    "y_pred = []\n",
    "\n",
    "error = []\n",
    "for i in range(len(df['image'])):\n",
    "    if i == 20:\n",
    "        break\n",
    "    image_path = df['image'][i]\n",
    "    class_id = df['class'][i]\n",
    "    \n",
    "    try:\n",
    "        image = scanner.process(image_utils.load(image_path))\n",
    "        image = image_utils.pre_process(image)\n",
    "        text = tesseract.excecute(image)\n",
    "    except:\n",
    "        error.append(image_path)\n",
    "        continue\n",
    "    \n",
    "    tokens = word_tokenize(text_utils.text_cleaner(text))\n",
    "    tokens = text_utils.fix_tokens(tokens, STOPWORDS)\n",
    "        \n",
    "    if len(tokens) >= LEN_TOKEN:\n",
    "        tokens = tokens[0:LEN_TOKEN]\n",
    "    else:\n",
    "        for i in range(len(tokens) - LEN_TOKEN):\n",
    "            tokens.append('None')\n",
    "    input_ids = torch.tensor([tokenizer.encode(tokens)])\n",
    "    with torch.no_grad():\n",
    "        features = phobert(input_ids) \n",
    "    es_dim = features['pooler_output'][0].tolist()\n",
    "    pred = es_utils.matching_elasticsearch_index(es, INDEX_NAME, es_dim)\n",
    "   \n",
    "    hits = pred['hits']['hits']\n",
    "    score = hits[0]['_score']/2\n",
    "    class_pre = hits[0]['_source']['id']\n",
    "    if score < 0.86:\n",
    "        print('special_case')\n",
    "        a = {'0': 0, '1':0, '2': 0}\n",
    "        for i in range(len(hits)):\n",
    "            a[str(i)] += 1\n",
    "        max_end = max(a, key=a.get)\n",
    "        class_pre = hits[int(max_end)]['_source']['id']\n",
    "    \n",
    "    y_actu.append(class_id)\n",
    "    y_pred.append(class_pre)\n",
    "    \n",
    "    if class_id == class_pre:\n",
    "        x += 1\n",
    "    else:\n",
    "        print(image_path, class_pre, class_id)\n",
    "#     confusion_matrix[class_id -1, class_pre -1] += 1\n",
    "    print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "    \n",
    "print(x/(len(df['image']) - len(error)))\n",
    "pprint(error)\n",
    "\n",
    "# #     df_check = pd.json_normalize(hits)\n",
    "# #     df_check = df_check[['_source.id', '_score']]\n",
    "# #     df_check = df_check.rename(columns = {'_source.id': 'id', '_score': 'score'})\n",
    "# #     df_check = df_check.groupby([\"id\"], as_index=False).mean()\n",
    "# #     class_pre = df_check.loc[df_check['score'].idxmax()]['id']\n",
    "\n",
    "df_show= df['class'].apply(get_class_name)\n",
    "df_show.value_counts().plot(kind=\"bar\", title=\"Number of Test Data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08f59c43-37e3-4421-a7cd-ad72c341a6de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sn\n",
    "import numpy as np\n",
    "y = confusion_matrix(y_actu, y_pred)\n",
    "columns = []\n",
    "for key, value in CLASSES.items():\n",
    "    columns.append(value)\n",
    "y = y.astype(np.float32)\n",
    "df_cm = pd.DataFrame(y, index = columns,\n",
    "                     columns =  columns)\n",
    "plt.figure(figsize = (len(CLASSES), len(CLASSES)))\n",
    "sn.heatmap(df_cm, annot=True)\n",
    "\n",
    "for i in range(len(CLASSES)):\n",
    "    total = sum(y[i])\n",
    "    for j in range(len(CLASSES)):\n",
    "        y[i][j] = y[i][j]/total\n",
    "df_cm = pd.DataFrame(y, index = columns,\n",
    "                     columns =  columns)\n",
    "plt.figure(figsize = (len(CLASSES), len(CLASSES)))\n",
    "sn.heatmap(df_cm, annot=True)\n",
    "\n",
    "        \n",
    "import seaborn as sns\n",
    "def plot_confusion_matrix(test_y, predict_y, classes):\n",
    "    \"\"\"\n",
    "    This function plots the confusion matrix given predicted and actual values.\n",
    "    \"\"\"\n",
    "    C = confusion_matrix(test_y, predict_y)\n",
    "    # C = 9,9 matrix, each cell (i,j) represents number of points of class i are predicted class j\n",
    "    \n",
    "    A =(((C.T)/(C.sum(axis=1))).T)    \n",
    "    B =(C/C.sum(axis=0))\n",
    "    labels = classes\n",
    "    cmap=sns.light_palette(\"green\")\n",
    "    # representing A in heatmap format\n",
    "    plt.figure(figsize=(17,5))\n",
    "    sns.heatmap(C, annot=True, cmap=cmap, fmt=\".3f\", xticklabels=labels, yticklabels=labels)\n",
    "    plt.xlabel('Predicted Class')\n",
    "    plt.ylabel('Original Class')\n",
    "    plt.show()\n",
    "    print(\"-\"*50, \"Confusion matrix\", \"-\"*50)\n",
    "\n",
    "    plt.figure(figsize=(17,5))\n",
    "    sns.heatmap(B, annot=True, cmap=cmap, fmt=\".3f\", xticklabels=labels, yticklabels=labels)\n",
    "    plt.xlabel('Predicted Class')\n",
    "    plt.ylabel('Original Class')\n",
    "    plt.show()\n",
    "    print(\"-\"*50, \"Precision matrix\", \"-\"*50)\n",
    "    \n",
    "#     print(\"Sum of columns in precision matrix\",B.sum(axis=0))\n",
    "    \n",
    "    # representing B in heatmap format\n",
    "    plt.figure(figsize=(17,5))\n",
    "    sns.heatmap(A, annot=True, cmap=cmap, fmt=\".3f\", xticklabels=labels, yticklabels=labels)\n",
    "    plt.xlabel('Predicted Class')\n",
    "    plt.ylabel('Original Class')\n",
    "    plt.show()\n",
    "    print(\"-\"*50, \"Recall matrix\"    , \"-\"*50)\n",
    "#     print(\"Sum of rows in precision matrix\",A.sum(axis=1))\n",
    "plot_confusion_matrix(y_actu, y_pred, X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c7aa882-b12f-49fb-ae62-7d659b905a2d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
